{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import sklearn\n",
    "import sklearn.model_selection\n",
    "\n",
    "inputs = [] \n",
    "labels = []\n",
    "with open(\"seeds_dataset.txt\", \"r\") as f:\n",
    "    for line in f:\n",
    "        inputs.append([float(val) for val in line.strip().split()[:7]])\n",
    "        labels.append(int(line.strip().split()[7]))\n",
    "\n",
    "inputs = np.array(inputs, dtype=float)\n",
    "labels = np.array(labels)\n",
    "max_input = np.max(inputs)\n",
    "\n",
    "x_train, x_test, y_train, y_test = sklearn.model_selection.train_test_split(inputs, labels, train_size=0.8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(X):\n",
    "    exp_x = np.exp(X - np.max(X))\n",
    "    return exp_x / np.sum(exp_x)\n",
    "\n",
    "def softmax_deriv(X):\n",
    "    softmax_output = softmax(X)\n",
    "    return softmax_output * (1 - softmax_output)\n",
    "\n",
    "def relu(X):\n",
    "    return np.maximum(0, X)\n",
    "\n",
    "def relu_deriv(X):\n",
    "    return X > 0\n",
    "\n",
    "def get_expected_from_label(label):\n",
    "    expected = np.zeros(3)\n",
    "    expected[label - 1] = 1.0\n",
    "    return expected\n",
    "\n",
    "def mean_squared_error(output, expected_label):\n",
    "    expected = get_expected_from_label(expected_label)    \n",
    "\n",
    "    return np.mean((expected - output) ** 2)\n",
    "\n",
    "class Layer:\n",
    "    def __init__(self, m, n, activation_func):\n",
    "        #n - nr de neuroni de pe stratul x\n",
    "        #m - nr de neuroni de pe stratul x - 1\n",
    "        self.weight = np.random.rand(n, m)\n",
    "        self.bias = np.random.rand(n)\n",
    "        self.activation = activation_func\n",
    "    \n",
    "    def activate(self, neurons):\n",
    "        return self.activation(neurons)\n",
    "\n",
    "alfa = 0.01 #learning rate\n",
    "epochs = 10\n",
    "\n",
    "# 2 straturi ascunse de cate 5 neuroni fiecare.\n",
    "# Activare - ReLU pe straturile ascunse, Sigmoid pe cel de output\n",
    "layers = [Layer(7, 5, relu), Layer(5, 5, relu), Layer(5, 3, softmax)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20.24   16.91    0.8897  6.315   3.962   5.901   6.188 ]\n",
      "[36.9906582  39.30787341 36.87266602 33.87319345 37.8885839 ]\n",
      "[107.75190889  51.08343809  93.25721741  81.68064576  71.6824883 ]\n",
      "[1.38481543e-68 1.00000000e+00 4.48367950e-50]\n",
      "1.0\n",
      "MSE: 6.70112729710361e-100\n"
     ]
    }
   ],
   "source": [
    "def compute_output(input, layer):\n",
    "    output = np.dot(layer.weight, input) + layer.bias\n",
    "    return layer.activate(output)\n",
    "\n",
    "def feed_forward(input, label, layers):\n",
    "    output = None\n",
    "    for layer in layers:\n",
    "        print(input)\n",
    "        output = compute_output(input, layer)\n",
    "        input = output\n",
    "    print(output)\n",
    "    print(np.sum(output))\n",
    "    print(f\"MSE: {mean_squared_error(output, label)}\")\n",
    "\n",
    "feed_forward(x_train[2], y_train[2], layers)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
