{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import sklearn\n",
    "import sklearn.model_selection\n",
    "\n",
    "inputs = [] \n",
    "labels = []\n",
    "with open(\"seeds_dataset.txt\", \"r\") as f:\n",
    "    for line in f:\n",
    "        inputs.append([float(val) for val in line.strip().split()[:7]])\n",
    "        labels.append(int(line.strip().split()[7]))\n",
    "\n",
    "inputs = np.array(inputs, dtype=float)\n",
    "labels = np.array(labels)\n",
    "max_val = np.max(inputs)\n",
    "\n",
    "x_train, x_test, y_train, y_test = sklearn.model_selection.train_test_split(inputs, labels, train_size=0.8, shuffle=True)\n",
    "x_train = x_train / max_val\n",
    "x_test = x_test / max_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    sigmoid_x = 1 / (1 + np.exp(-x))\n",
    "    return sigmoid_x * (1 - sigmoid_x)\n",
    "\n",
    "def relu_derivative(x):\n",
    "    return np.where(x > 0, 1, 0)\n",
    "\n",
    "def mean_squared_error(output, expected_label):\n",
    "    expected = np.zeros(3)\n",
    "    expected[expected_label - 1] = 1.0\n",
    "    print(output, expected)\n",
    "\n",
    "    return 1 / 2 * np.sum((expected - output) ** 2)\n",
    "\n",
    "class Layer:\n",
    "    def __init__(self, m, n, activation):\n",
    "        #n - nr de neuroni de pe stratul x\n",
    "        #m - nr de neuroni de pe stratul x - 1\n",
    "        self.matrix = np.random.rand(n, m)\n",
    "        self.activation = activation\n",
    "    \n",
    "    def activate(self, neurons):\n",
    "        output = []\n",
    "        for neuron in neurons:\n",
    "            output.append(self.activation(neuron))\n",
    "        return np.array(output) \n",
    "\n",
    "alfa = 0.01 #learning rate\n",
    "epochs = 10\n",
    "\n",
    "# 2 straturi ascunse de cate 5 neuroni fiecare.\n",
    "# Activare - ReLU pe straturile ascunse, Sigmoid pe cel de output\n",
    "layers = [Layer(7, 5, relu), Layer(5, 5, relu), Layer(5, 3, sigmoid)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.67705382 0.67847025 0.04119924 0.26581681 0.15061379 0.06199245\n",
      " 0.24315392]\n",
      "[1.56077003 1.10196043 0.852695   1.23863542 0.60598857]\n",
      "[2.07669807 3.24708953 2.35069905 4.14987464 2.63522775]\n",
      "[0.99980945 0.99985221 0.99821526]\n",
      "[0.99980945 0.99985221 0.99821526] [1. 0. 0.]\n",
      "MSE: 0.9980690946262576\n"
     ]
    }
   ],
   "source": [
    "def compute_output(input, layer):\n",
    "    output = np.dot(layer.matrix, input)\n",
    "    return layer.activate(output)\n",
    "\n",
    "def feed_forward(input, label, layers):\n",
    "    output = None\n",
    "    for layer in layers:\n",
    "        print(input)\n",
    "        output = compute_output(input, layer)\n",
    "        input = output\n",
    "    print(output)\n",
    "    print(f\"MSE: {mean_squared_error(output, label)}\")\n",
    "\n",
    "feed_forward(x_train[5], y_train[5], layers)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
